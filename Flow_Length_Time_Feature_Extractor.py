
import ujson as json
import sys
import gzip
from collections import defaultdict
import numpy as np
import pathlib
import xlsxwriter
import openpyxl
from openpyxl import load_workbook
from openpyxl import Workbook
import math
import statistics
import os
import pandas as pd
import csv
import xlsxwriter
import openpyxl
# from TLS_And_Meta_Features import *


def getByteDist(flow):
    if len(flow['packets']) == 0:
        return list(np.zeros(256))
    if 'byte_dist' in flow and sum(flow['byte_dist']) > 0:
        tmp = map(lambda x: x/float(sum(flow['byte_dist'])), flow['byte_dist'])
        rounded_digit= [round(num, 2) for num in list(tmp)]
        return rounded_digit
#         return list(tmp)
    else:
        return list(np.zeros(256))
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
def Sequence_of_Packet_Times(flow):
    numRows = 3
    binSize = 100.0
    transMat = np.zeros((numRows,numRows))
    time_sequence=[]
    if len(flow['packets']) == 0:
        return list(transMat.flatten())
    elif len(flow['packets']) == 1:
        cur = min(int(flow['packets'][0]['ipt']/float(binSize)), numRows-1)
        transMat[cur, cur] = 1
        return list(transMat.flatten())
    # get raw transition counts
    for i in range(1, len(flow['packets'])):
        prev = min(int(flow['packets'][i-1]['ipt']/float(binSize)), numRows-1)
        # print(flow['packets'][i]['ipt']/float(binSize))
        cur = min(int(flow['packets'][i]['ipt']/float(binSize)), numRows-1) 
        current=round(cur,2)
        previous=round(prev,2)
        transMat[previous, current] += 1
        # print(prev,"....",cur)
        # print(transMat)
    # get empirical transition probabilities
    for i in range(numRows):
        # print(float(np.sum(transMat[i:i+1])) )
        if float(np.sum(transMat[i:i+1])) != 0:
            transMat[i:i+1] = transMat[i:i+1]/float(np.sum(transMat[i:i+1]))
    rounded_digit= [round(num, 2) for num in list(transMat.flatten())]
    # print(rounded_digit)
    for x in range(len(rounded_digit)):
        if str(rounded_digit[x])=='0.0':
            time_sequence.append(0)
        else:
            time_sequence.append(rounded_digit[x])
    # print(time_sequence)
    return time_sequence


def Sequence_of_Packet_Lengths(flow):
    numRows = 3
    binSize = 300.0
    packet_length=[]
    transMat = np.zeros((numRows,numRows))
    if len(flow['packets']) == 0:
        return list(transMat.flatten())
    elif len(flow['packets']) == 1:
        cur = min(int(flow['packets'][0]['b']/float(binSize)), numRows-1)
        transMat[cur, cur] = 1
        return list(transMat.flatten())
    # get raw transition counts
    for i in range(1, len(flow['packets'])):
        prev = min(int(flow['packets'][i-1]['b']/float(binSize)), numRows-1)
        #if 'b' not in flow['packets'][i]:
        #break
        cur = min(int(flow['packets'][i]['b']/float(binSize)), numRows-1)
        transMat[prev, cur] += 1
    # get empirical transition probabilities
    for i in range(numRows):
        if float(np.sum(transMat[i:i+1])) != 0:
            transMat[i:i+1] = transMat[i:i+1]/float(np.sum(transMat[i:i+1]))
    rounded_digit= [round(num, 2) for num in list(transMat.flatten())]
    for x in range(len(rounded_digit)):
        if str(rounded_digit[x])=='0.0':
            packet_length.append(0)
        else:
            packet_length.append(rounded_digit[x])
    # print(packet_length)
    return packet_length    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
def getTimes(flow):
    numRows = 3
    binSize = 100.0
    transMat = np.zeros((numRows,numRows))
    time_sequence=[]
    if len(flow['packets']) == 0:
        return list(transMat.flatten())
    elif len(flow['packets']) == 1:
        cur = min(int(flow['packets'][0]['ipt']/float(binSize)), numRows-1)
        transMat[cur, cur] = 1
        return list(transMat.flatten())
    # get raw transition counts
    for i in range(1, len(flow['packets'])):
        prev = min(int(flow['packets'][i-1]['ipt']/float(binSize)), numRows-1)
        # print(flow['packets'][i]['ipt']/float(binSize))
        cur = min(int(flow['packets'][i]['ipt']/float(binSize)), numRows-1) 
        current=round(cur,2)
        previous=round(prev,2)
        transMat[previous, current] += 1
        print(prev,"....",cur)
        print(transMat)
    # get empirical transition probabilities
    for i in range(numRows):
        # print(float(np.sum(transMat[i:i+1])) )
        if float(np.sum(transMat[i:i+1])) != 0:
            transMat[i:i+1] = transMat[i:i+1]/float(np.sum(transMat[i:i+1]))
    rounded_digit= [round(num, 2) for num in list(transMat.flatten())]
    print(rounded_digit)
    for x in range(len(rounded_digit)):
        if str(rounded_digit[x])=='0.0':
            time_sequence.append(0)
        else:
            time_sequence.append(rounded_digit[x])
    print(time_sequence)
    return time_sequence


def getLengths(flow):
    numRows = 3
    binSize = 300.0
    packet_length=[]
    transMat = np.zeros((numRows,numRows))
    if len(flow['packets']) == 0:
        return list(transMat.flatten())
    elif len(flow['packets']) == 1:
        cur = min(int(flow['packets'][0]['b']/float(binSize)), numRows-1)
        transMat[cur, cur] = 1
        return list(transMat.flatten())
    # get raw transition counts
    for i in range(1, len(flow['packets'])):
        prev = min(int(flow['packets'][i-1]['b']/float(binSize)), numRows-1)
        #if 'b' not in flow['packets'][i]:
        #break
        cur = min(int(flow['packets'][i]['b']/float(binSize)), numRows-1)
        transMat[prev, cur] += 1
    # get empirical transition probabilities
    for i in range(numRows):
        if float(np.sum(transMat[i:i+1])) != 0:
            transMat[i:i+1] = transMat[i:i+1]/float(np.sum(transMat[i:i+1]))
    rounded_digit= [round(num, 2) for num in list(transMat.flatten())]
    for x in range(len(rounded_digit)):
        if str(rounded_digit[x])=='0.0':
            packet_length.append(0)
        else:
            packet_length.append(rounded_digit[x])
    # print(packet_length)
    return packet_length
#     return list(transMat.flatten())

def CSVWriter(row): 
    file='AASTU_Normal.csv' 
    Folder='/home/elliot/Desktop/CSV_Files'
    os.chdir(Folder)
    Files = os.listdir(Folder) 
    if file in Files:
        with open(file, 'a', newline='') as f_object:
              writer = csv.writer(f_object) 
              writer.writerow(row) 
              f_object.close()
    else:
          file=open(file, 'w', newline='')
          with open(file, 'a', newline='') as f_object:
            writer = csv.writer(f_object) 
            writer.writerow(row) 
            f_object.close()        
            
def ProcessPacketSequence(inPathName, fileName, meta):
    json_file = "%s%s" % (inPathName, fileName)
    lineno = 0 
    with gzip.open(json_file, 'r') as fp:
        server_address_holder=[]  
        i=0
        total_flow_count=1
        serverAddr_holder=[]
        for line in fp:  
            lineno = lineno + 1
            tmp = json.loads(line)
            if ('version' in tmp) or ("tls" not in tmp) or (int(tmp["dp"]) != 443):
                continue
            if ('version' not in tmp) and 'tls' in tmp   and (int(tmp["dp"]) == 443):
                serverAddr = "%s__%s__%s"%(tmp["sa"],str(tmp["sp"]),tmp["da"])
                resp = tmp["tls"] 
                if total_flow_count>1:
                    continue
                else:    
                    print("\nAnother Flow:\n")
                    if('s_cert' not in resp or serverAddr in serverAddr_holder):
                        continue
                    else:
                        serverAddr_holder.append(serverAddr) 
                        meta[serverAddr] = defaultdict()
                        
                        #1 times
                        meta[serverAddr]['flowTimes'] = getTimes(tmp)
                        flow_times_Flatten=meta[serverAddr]['flowTimes']
                        
                        #2 lengths
                        meta[serverAddr]['flowLengths'] = getLengths(tmp)
                        flow_length_Flatten=meta[serverAddr]['flowLengths']
                        meta[serverAddr]['flowByteDist'] = getByteDist(tmp)
                        
                        # meta_features=getMetadata(tmp)
                        Splt_Flatten=flow_length_Flatten+flow_times_Flatten
                        i=i+1
                        stringI=str(i)
                        fileName, file_extension=os.path.splitext(fileName)
                        fileNames=str(fileName+'__'+stringI) 
                        # CSVWriter(Splt_Flatten,"\n") 
                    # print(Splt_Flatten,"\n")
                total_flow_count+=1
   # return Spl t_Flatten
# print("Meta features processed")


def saveToJson(outPathName, fileName, meta):
    fname = "%s%s.json" % (outPathName, (fileName.split('.'))[0])  
    with open(fname, "w") as fp:
        json.dump(meta,fp)


def main():
    inputF='/home/elliot/Desktop/Json_Files/MTA-2020-2021/1/'
    #setup input folder and output folders
    Files = os.listdir(inputF)
    Files.sort()
    json=True
    print("Total Number of Files :",len(Files))    
    if inputF == None or not os.path.isdir(inputF):
        print("No valid input folder!")
        return
    else:
        joyFolder = inputF
        if not joyFolder.endswith('/'):
            joyFolder += '/'
    parentFolder = os.path.abspath(os.path.join(joyFolder, os.pardir))
    if not parentFolder.endswith('/'):
        parentFolder += '/'
    META_JSON_Folder = "%sMETA_JSON/" % (parentFolder)
    if not os.path.exists(META_JSON_Folder):
        os.mkdir(META_JSON_Folder)
    if json == True:
        files = os.listdir(joyFolder)
        for item in files:
            try:
                meta = defaultdict()
                tls = defaultdict()
                print("Processing SPLT Feature of %s"%item)  
                Tls_meta_fea=[] 
                # Tls_meta_fea=Analyze_TLS(joyFolder, item, tls) 
                ProcessPacketSequence(joyFolder, item, meta) 
                saveToJson(META_JSON_Folder, item, meta)
            except:
                continue
    else:
        print("Nothing to do!")
        return
    print("Flow Length Features Extracted!!")
    print("Flow Length Features Stored in Excel")
    # excelAggregator()
    print("Flow Length Data Aggregating in Excel")
    print("\nProcess Done ")
    
if __name__ == "__main__":
    main()
    

# def toExcel(data,fileName): 
#         excel_Dir='/home/elliot/Desktop/Thesis/Main_Features/Excel_Output_Folder/'
#         filename, file_extension=os.path.splitext(fileName)
#         xlsfile = excel_Dir+filename+'.xlsx'
#         my_list = [data]
#         ex=str(xlsfile) 
#         file = pathlib.Path(ex)
#         if file.exists ():
#             wb = load_workbook(ex)
#             worksheet = wb.add_worksheet()
#             print ("File exist")        
#             for row_num, row_data in enumerate(my_list):
#                 for col_num, col_data in enumerate(row_data):
#                     worksheet.write(row_num, col_num, col_data)
#             wb.close()
#         else:
#             workbook = xlsxwriter.Workbook(ex)
#             worksheet = workbook.add_worksheet()
#             for row_num, row_data in enumerate(my_list):
#                 for col_num, col_data in enumerate(row_data):
#                     worksheet.write(row_num, col_num, col_data)
#             workbook.close()
# def excelAggregator():
#     cwd = os.chdir('/home/elliot/Desktop/Thesis/Main_Features/Excel_Output_Folder/')
#     Files = os.listdir('/home/elliot/Desktop/Thesis/Main_Features/Excel_Output_Folder') 
#     Files.sort()
#     excel_names=[]
#     for file in Files:
#         if file.endswith('.xlsx'):
#                 excel_names.append(file)
# #     print("Total Excel files :",len(excel_names))
#     files = os.listdir(cwd) 
#     files.sort()    
#     direc='/home/elliot/Desktop/Thesis/Main_Features/Excel_Output_Folder/'
#     excels = [pd.ExcelFile(str(direc+name)) for name in excel_names]
#     frames = [x.parse(x.sheet_names[0], header=None,index_col=None) for x in excels]
#     frames[0:] = [df[0:] for df in frames[0:]]
#     combined = pd.concat(frames)
#     combined.to_excel("SPLT.xlsx", header=False, index=False)
    
#     # for f in Files:
#     #     if f=='SPLT.xlsx' or f=='Meta_and_SPLT_Features.xlsx':
#     #         continue
#     #     else:
#     #         os.remove(os.path.join('/home/elliot/Desktop/Thesis/Main_Features/Excel_Output_Folder', f))            
            
            
            

